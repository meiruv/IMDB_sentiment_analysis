{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis with Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZQRrkm4SM0tKPwbzvcMhE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meiruv/IMDB_sentiment_analysis/blob/main/Sentiment_Analysis_with_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsJaataIaoLA",
        "outputId": "9805ecbb-6a83-4dd3-cb7e-d7882dfe9ee5"
      },
      "source": [
        "!pip install torch==1.6.0 torchvision==0.7.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Requirement already satisfied: torchvision==0.7.0 in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8FTyoNjxIwh",
        "outputId": "d383b9ee-2130-4d60-f4b6-8b61c8f0fc16"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuG9tnFjzUdz"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jajy0brwzi1I"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD73oOFDzwNA",
        "outputId": "7b061e0b-3222-49ea-d373-0ef6e231a5a3"
      },
      "source": [
        "stop_words"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVwnE4PszxNm"
      },
      "source": [
        "# we disable those functions because otherwise they will be performed automatically\n",
        "# everytime we run nlp on a text which massively slows down runtime.\n",
        "nlp = spacy.load(\"en\",disable=['parser', 'tagger', 'ner'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN9pNoV24jiL"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPqG5vf0z3PW"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN3lMrGR0sQS",
        "outputId": "5e4906de-ee6c-46a9-e9d0-d9d7b27a35b9"
      },
      "source": [
        "reviewsFile = open('reviews.txt','r')\n",
        "reviews = list(map(lambda x:x[:-1],reviewsFile.readlines()))\n",
        "reviewsFile.close()\n",
        "\n",
        "labelsFile = open('labels.txt','r')\n",
        "labels = list(map(lambda x:x[:-1],labelsFile.readlines()))\n",
        "labelsFile.close()\n",
        "\n",
        "print('# of reviews: ', len(reviews))\n",
        "print('# of labels: ', len(labels))\n",
        "print(set(labels))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of reviews:  25000\n",
            "# of labels:  25000\n",
            "{'positive', 'negative'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6MZ5zQfT1tk",
        "outputId": "f95c2993-447a-40b5-869b-db8e45b1e206"
      },
      "source": [
        "labelsFile = open('labels.txt','r')\n",
        "labels = list(map(lambda x:x[:-1],labelsFile.readlines()))\n",
        "labelsFile.close()\n",
        "\n",
        "labels[:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['positive', 'negative', 'positive', 'negative', 'positive']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "l05HPWBM1cMo",
        "outputId": "4069eaa5-c774-45fb-cdf8-6ab10e2599b1"
      },
      "source": [
        "print('# of reviews: ', len(reviews))\n",
        "print('# of labels: ', len(labels))\n",
        "print(set(labels))\n",
        "reviews[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of reviews:  25000\n",
            "# of labels:  25000\n",
            "{'positive', 'negative'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   '"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9cQD6aL4mvb"
      },
      "source": [
        "## Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myrphWUr151h",
        "outputId": "77e08389-f355-4b02-9a09-c91d8cd0d091"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIcOQeme4gwr"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_co35o-1qP_",
        "outputId": "6d3a4a3a-3485-4dd6-93b7-28f6d298f515"
      },
      "source": [
        "nltk.word_tokenize(reviews[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bromwell',\n",
              " 'high',\n",
              " 'is',\n",
              " 'a',\n",
              " 'cartoon',\n",
              " 'comedy',\n",
              " '.',\n",
              " 'it',\n",
              " 'ran',\n",
              " 'at',\n",
              " 'the',\n",
              " 'same',\n",
              " 'time',\n",
              " 'as',\n",
              " 'some',\n",
              " 'other',\n",
              " 'programs',\n",
              " 'about',\n",
              " 'school',\n",
              " 'life',\n",
              " 'such',\n",
              " 'as',\n",
              " 'teachers',\n",
              " '.',\n",
              " 'my',\n",
              " 'years',\n",
              " 'in',\n",
              " 'the',\n",
              " 'teaching',\n",
              " 'profession',\n",
              " 'lead',\n",
              " 'me',\n",
              " 'to',\n",
              " 'believe',\n",
              " 'that',\n",
              " 'bromwell',\n",
              " 'high',\n",
              " 's',\n",
              " 'satire',\n",
              " 'is',\n",
              " 'much',\n",
              " 'closer',\n",
              " 'to',\n",
              " 'reality',\n",
              " 'than',\n",
              " 'is',\n",
              " 'teachers',\n",
              " '.',\n",
              " 'the',\n",
              " 'scramble',\n",
              " 'to',\n",
              " 'survive',\n",
              " 'financially',\n",
              " 'the',\n",
              " 'insightful',\n",
              " 'students',\n",
              " 'who',\n",
              " 'can',\n",
              " 'see',\n",
              " 'right',\n",
              " 'through',\n",
              " 'their',\n",
              " 'pathetic',\n",
              " 'teachers',\n",
              " 'pomp',\n",
              " 'the',\n",
              " 'pettiness',\n",
              " 'of',\n",
              " 'the',\n",
              " 'whole',\n",
              " 'situation',\n",
              " 'all',\n",
              " 'remind',\n",
              " 'me',\n",
              " 'of',\n",
              " 'the',\n",
              " 'schools',\n",
              " 'i',\n",
              " 'knew',\n",
              " 'and',\n",
              " 'their',\n",
              " 'students',\n",
              " '.',\n",
              " 'when',\n",
              " 'i',\n",
              " 'saw',\n",
              " 'the',\n",
              " 'episode',\n",
              " 'in',\n",
              " 'which',\n",
              " 'a',\n",
              " 'student',\n",
              " 'repeatedly',\n",
              " 'tried',\n",
              " 'to',\n",
              " 'burn',\n",
              " 'down',\n",
              " 'the',\n",
              " 'school',\n",
              " 'i',\n",
              " 'immediately',\n",
              " 'recalled',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " 'at',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " 'high',\n",
              " '.',\n",
              " 'a',\n",
              " 'classic',\n",
              " 'line',\n",
              " 'inspector',\n",
              " 'i',\n",
              " 'm',\n",
              " 'here',\n",
              " 'to',\n",
              " 'sack',\n",
              " 'one',\n",
              " 'of',\n",
              " 'your',\n",
              " 'teachers',\n",
              " '.',\n",
              " 'student',\n",
              " 'welcome',\n",
              " 'to',\n",
              " 'bromwell',\n",
              " 'high',\n",
              " '.',\n",
              " 'i',\n",
              " 'expect',\n",
              " 'that',\n",
              " 'many',\n",
              " 'adults',\n",
              " 'of',\n",
              " 'my',\n",
              " 'age',\n",
              " 'think',\n",
              " 'that',\n",
              " 'bromwell',\n",
              " 'high',\n",
              " 'is',\n",
              " 'far',\n",
              " 'fetched',\n",
              " '.',\n",
              " 'what',\n",
              " 'a',\n",
              " 'pity',\n",
              " 'that',\n",
              " 'it',\n",
              " 'isn',\n",
              " 't']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhYllJsJ10ZG"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(\"\\w+\\'?\\w+|\\w+\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jlig-Of2BJm",
        "outputId": "b36be8c6-cda5-424e-e996-13a7bb79c63a"
      },
      "source": [
        "tokenizer.tokenize(\"I didn't like the movie.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', \"didn't\", 'like', 'the', 'movie']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdLv04HQ3AOm",
        "outputId": "48dc23df-589a-44ee-ec77-70530809a43c"
      },
      "source": [
        "tokenizer.tokenize(reviews[0])\n",
        "# ~~~~~~~~~~~~~~~~~ but this did split the 'isn't' in the end of the review so I think this regex is wrong... ~~~~~~~~~~~~~~~~~"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bromwell',\n",
              " 'high',\n",
              " 'is',\n",
              " 'a',\n",
              " 'cartoon',\n",
              " 'comedy',\n",
              " 'it',\n",
              " 'ran',\n",
              " 'at',\n",
              " 'the',\n",
              " 'same',\n",
              " 'time',\n",
              " 'as',\n",
              " 'some',\n",
              " 'other',\n",
              " 'programs',\n",
              " 'about',\n",
              " 'school',\n",
              " 'life',\n",
              " 'such',\n",
              " 'as',\n",
              " 'teachers',\n",
              " 'my',\n",
              " 'years',\n",
              " 'in',\n",
              " 'the',\n",
              " 'teaching',\n",
              " 'profession',\n",
              " 'lead',\n",
              " 'me',\n",
              " 'to',\n",
              " 'believe',\n",
              " 'that',\n",
              " 'bromwell',\n",
              " 'high',\n",
              " 's',\n",
              " 'satire',\n",
              " 'is',\n",
              " 'much',\n",
              " 'closer',\n",
              " 'to',\n",
              " 'reality',\n",
              " 'than',\n",
              " 'is',\n",
              " 'teachers',\n",
              " 'the',\n",
              " 'scramble',\n",
              " 'to',\n",
              " 'survive',\n",
              " 'financially',\n",
              " 'the',\n",
              " 'insightful',\n",
              " 'students',\n",
              " 'who',\n",
              " 'can',\n",
              " 'see',\n",
              " 'right',\n",
              " 'through',\n",
              " 'their',\n",
              " 'pathetic',\n",
              " 'teachers',\n",
              " 'pomp',\n",
              " 'the',\n",
              " 'pettiness',\n",
              " 'of',\n",
              " 'the',\n",
              " 'whole',\n",
              " 'situation',\n",
              " 'all',\n",
              " 'remind',\n",
              " 'me',\n",
              " 'of',\n",
              " 'the',\n",
              " 'schools',\n",
              " 'i',\n",
              " 'knew',\n",
              " 'and',\n",
              " 'their',\n",
              " 'students',\n",
              " 'when',\n",
              " 'i',\n",
              " 'saw',\n",
              " 'the',\n",
              " 'episode',\n",
              " 'in',\n",
              " 'which',\n",
              " 'a',\n",
              " 'student',\n",
              " 'repeatedly',\n",
              " 'tried',\n",
              " 'to',\n",
              " 'burn',\n",
              " 'down',\n",
              " 'the',\n",
              " 'school',\n",
              " 'i',\n",
              " 'immediately',\n",
              " 'recalled',\n",
              " 'at',\n",
              " 'high',\n",
              " 'a',\n",
              " 'classic',\n",
              " 'line',\n",
              " 'inspector',\n",
              " 'i',\n",
              " 'm',\n",
              " 'here',\n",
              " 'to',\n",
              " 'sack',\n",
              " 'one',\n",
              " 'of',\n",
              " 'your',\n",
              " 'teachers',\n",
              " 'student',\n",
              " 'welcome',\n",
              " 'to',\n",
              " 'bromwell',\n",
              " 'high',\n",
              " 'i',\n",
              " 'expect',\n",
              " 'that',\n",
              " 'many',\n",
              " 'adults',\n",
              " 'of',\n",
              " 'my',\n",
              " 'age',\n",
              " 'think',\n",
              " 'that',\n",
              " 'bromwell',\n",
              " 'high',\n",
              " 'is',\n",
              " 'far',\n",
              " 'fetched',\n",
              " 'what',\n",
              " 'a',\n",
              " 'pity',\n",
              " 'that',\n",
              " 'it',\n",
              " 'isn',\n",
              " 't']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On6jsHVA3EGm"
      },
      "source": [
        "reviews = list(map(lambda x:tokenizer.tokenize(x.lower()),reviews))\n",
        "# ~~~~~~~~~~~~~~~~  why did I need to use a map function here? ~~~~~~~~~~~~~~~~~~~~~~~~~~~"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q_jom4Y4tTA"
      },
      "source": [
        "## Stop Words\n",
        "\n",
        "I will use both spacy and nltk's cololection of stop words since they have different ones.\n",
        "It is important to note that NLTK's collection is list and spaCy's is a set. \n",
        "I'll convert NLTK stop words collection into a set and perform a union between these sets so that we have a set of unique stop words. The reason I prefer sets over lists is that sets are significantly faster than a list to check whether a given word is in it compared to a list. However lists are faster than sets where there is an iteration to be performed on the contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbk2Kp2T3cNF"
      },
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4FvwMA25oZ_"
      },
      "source": [
        "exceptionStopWords = {\n",
        "    'again',\n",
        "    'against',\n",
        "    'ain',\n",
        "    'almost',\n",
        "    'among',\n",
        "    'amongst',\n",
        "    'amount',\n",
        "    'anyhow',\n",
        "    'anyway',\n",
        "    'aren',\n",
        "    \"aren't\",\n",
        "    'below',\n",
        "    'bottom',\n",
        "    'but',\n",
        "    'cannot',\n",
        "    'couldn',\n",
        "    \"couldn't\",\n",
        "    'didn',\n",
        "    \"didn't\",\n",
        "    'doesn',\n",
        "    \"doesn't\",\n",
        "    'don',\n",
        "    \"don't\",\n",
        "    'done',\n",
        "    'down',\n",
        "    'except',\n",
        "    'few',\n",
        "    'hadn',\n",
        "    \"hadn't\",\n",
        "    'hasn',\n",
        "    \"hasn't\",\n",
        "    'haven',\n",
        "    \"haven't\",\n",
        "    'however',\n",
        "    'isn',\n",
        "    \"isn't\",\n",
        "    'least',\n",
        "    'mightn',\n",
        "    \"mightn't\",\n",
        "    'move',\n",
        "    'much',\n",
        "    'must',\n",
        "    'mustn',\n",
        "    \"mustn't\",\n",
        "    'needn',\n",
        "    \"needn't\",\n",
        "    'neither',\n",
        "    'never',\n",
        "    'nevertheless',\n",
        "    'no',\n",
        "    'nobody',\n",
        "    'none',\n",
        "    'noone',\n",
        "    'nor',\n",
        "    'not',\n",
        "    'nothing',\n",
        "    'should',\n",
        "    \"should've\",\n",
        "    'shouldn',\n",
        "    \"shouldn't\",\n",
        "    'too',\n",
        "    'top',\n",
        "    'up',\n",
        "    'wasn',\n",
        "    \"wasn't\",\n",
        "    'well',\n",
        "    'weren',\n",
        "    \"weren't\",\n",
        "    'won',\n",
        "    \"won't\",\n",
        "    'wouldn',\n",
        "    \"wouldn't\",\n",
        "}"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8j9WMBZ51jO"
      },
      "source": [
        "stop_words = set(stop_words).union(STOP_WORDS)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M0GlUbi54sm"
      },
      "source": [
        "final_stop_words = stop_words-exceptionStopWords\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2uINSq856c2"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM3JaHAZ_05v"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatization is a process in linguistics for grouping together the inflected forms of a word, so that they can be analyzed as a single item identified by the words lemma or dictionary form. What this means that words like is our B would be reduced to B, or words like walk walking walked would be reduced to walk, that is the words would be reduced to their base form. It-\n",
        "- Reduces the feature space\n",
        "- Allows more precise scores for methods like BOW, tf-idf etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piu5Re4kBXTu"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMIyuAoXDU2J"
      },
      "source": [
        "# Entire Preprocessing With Functions and Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CcGNN_nDZgB"
      },
      "source": [
        "def load_data():\n",
        "    reviewsFile = open('reviews.txt','r')\n",
        "    reviews = list(map(lambda x:x[:-1],reviewsFile.readlines()))\n",
        "    reviewsFile.close()\n",
        "\n",
        "    labelsFile = open('labels.txt','r')\n",
        "    labels = list(map(lambda x:x[:-1],labelsFile.readlines()))\n",
        "    labelsFile.close()\n",
        "    \n",
        "    return reviews,labels"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha1BHcemDcdG"
      },
      "source": [
        "reviews,labels = load_data()\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rezRCu5uDeMl"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(\"\\w+\\'?\\w+|\\w+\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIVc3YCeDhw9"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbA2p7a0Dhzk"
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85nsB525Dh2a"
      },
      "source": [
        "exceptionStopWords = {\n",
        "    'again',\n",
        "    'against',\n",
        "    'ain',\n",
        "    'almost',\n",
        "    'among',\n",
        "    'amongst',\n",
        "    'amount',\n",
        "    'anyhow',\n",
        "    'anyway',\n",
        "    'aren',\n",
        "    \"aren't\",\n",
        "    'below',\n",
        "    'bottom',\n",
        "    'but',\n",
        "    'cannot',\n",
        "    'couldn',\n",
        "    \"couldn't\",\n",
        "    'didn',\n",
        "    \"didn't\",\n",
        "    'doesn',\n",
        "    \"doesn't\",\n",
        "    'don',\n",
        "    \"don't\",\n",
        "    'done',\n",
        "    'down',\n",
        "    'except',\n",
        "    'few',\n",
        "    'hadn',\n",
        "    \"hadn't\",\n",
        "    'hasn',\n",
        "    \"hasn't\",\n",
        "    'haven',\n",
        "    \"haven't\",\n",
        "    'however',\n",
        "    'isn',\n",
        "    \"isn't\",\n",
        "    'least',\n",
        "    'mightn',\n",
        "    \"mightn't\",\n",
        "    'move',\n",
        "    'much',\n",
        "    'must',\n",
        "    'mustn',\n",
        "    \"mustn't\",\n",
        "    'needn',\n",
        "    \"needn't\",\n",
        "    'neither',\n",
        "    'never',\n",
        "    'nevertheless',\n",
        "    'no',\n",
        "    'nobody',\n",
        "    'none',\n",
        "    'noone',\n",
        "    'nor',\n",
        "    'not',\n",
        "    'nothing',\n",
        "    'should',\n",
        "    \"should've\",\n",
        "    'shouldn',\n",
        "    \"shouldn't\",\n",
        "    'too',\n",
        "    'top',\n",
        "    'up',\n",
        "    'wasn',\n",
        "    \"wasn't\",\n",
        "    'well',\n",
        "    'weren',\n",
        "    \"weren't\",\n",
        "    'won',\n",
        "    \"won't\",\n",
        "    'wouldn',\n",
        "    \"wouldn't\",\n",
        "}"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A99B9zHRDh4W"
      },
      "source": [
        "stop_words = set(stop_words).union(STOP_WORDS)\n",
        "\n",
        "final_stop_words = stop_words-exceptionStopWords\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRw0LpCXDxZv"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en\",disable=['parser', 'tagger', 'ner'])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71kmaN_eDxcs"
      },
      "source": [
        "def make_token(review):\n",
        "    return tokenizer.tokenize(str(review))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8s_9XUiDxgK"
      },
      "source": [
        "def remove_stopwords(review):\n",
        "    return [token for token in review if token not in final_stop_words]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWMALYG7Dxm5"
      },
      "source": [
        "def lemmatization(review):\n",
        "    lemma_result = []\n",
        "    \n",
        "    for words in review:\n",
        "        doc = nlp(words)\n",
        "        for token in doc:\n",
        "            lemma_result.append(token.lemma_)\n",
        "    return lemma_result"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__MB4tX7Dh6t"
      },
      "source": [
        "def pipeline(review):\n",
        "    review = make_token(review)\n",
        "    review = remove_stopwords(review)\n",
        "    return lemmatization(review)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcLT99fvD_y0",
        "outputId": "70286387-6ba2-44f6-a40c-15ec4da9bd5c"
      },
      "source": [
        "%%time\n",
        "reviews = list(map(lambda review: pipeline(review),reviews))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 6s, sys: 658 ms, total: 2min 6s\n",
            "Wall time: 2min 6s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLpKsriIECnt",
        "outputId": "be132271-966a-45ae-b478-f38e18efb26d"
      },
      "source": [
        "reviews[:2]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['bromwell',\n",
              "  'high',\n",
              "  'cartoon',\n",
              "  'comedy',\n",
              "  'run',\n",
              "  'time',\n",
              "  'program',\n",
              "  'school',\n",
              "  'life',\n",
              "  'teacher',\n",
              "  'year',\n",
              "  'teach',\n",
              "  'profession',\n",
              "  'lead',\n",
              "  'believe',\n",
              "  'bromwell',\n",
              "  'high',\n",
              "  'satire',\n",
              "  'much',\n",
              "  'close',\n",
              "  'reality',\n",
              "  'teacher',\n",
              "  'scramble',\n",
              "  'survive',\n",
              "  'financially',\n",
              "  'insightful',\n",
              "  'student',\n",
              "  'right',\n",
              "  'pathetic',\n",
              "  'teacher',\n",
              "  'pomp',\n",
              "  'pettiness',\n",
              "  'situation',\n",
              "  'remind',\n",
              "  'school',\n",
              "  'know',\n",
              "  'student',\n",
              "  'see',\n",
              "  'episode',\n",
              "  'student',\n",
              "  'repeatedly',\n",
              "  'try',\n",
              "  'burn',\n",
              "  'down',\n",
              "  'school',\n",
              "  'immediately',\n",
              "  'recall',\n",
              "  'high',\n",
              "  'classic',\n",
              "  'line',\n",
              "  'inspector',\n",
              "  'sack',\n",
              "  'teacher',\n",
              "  'student',\n",
              "  'welcome',\n",
              "  'bromwell',\n",
              "  'high',\n",
              "  'expect',\n",
              "  'adult',\n",
              "  'age',\n",
              "  'think',\n",
              "  'bromwell',\n",
              "  'high',\n",
              "  'far',\n",
              "  'fetch',\n",
              "  'pity',\n",
              "  'isn'],\n",
              " ['story',\n",
              "  'man',\n",
              "  'unnatural',\n",
              "  'feeling',\n",
              "  'pig',\n",
              "  'start',\n",
              "  'open',\n",
              "  'scene',\n",
              "  'terrific',\n",
              "  'example',\n",
              "  'absurd',\n",
              "  'comedy',\n",
              "  'formal',\n",
              "  'orchestra',\n",
              "  'audience',\n",
              "  'turn',\n",
              "  'insane',\n",
              "  'violent',\n",
              "  'mob',\n",
              "  'crazy',\n",
              "  'chantings',\n",
              "  'singer',\n",
              "  'unfortunately',\n",
              "  'stay',\n",
              "  'absurd',\n",
              "  'time',\n",
              "  'no',\n",
              "  'general',\n",
              "  'narrative',\n",
              "  'eventually',\n",
              "  'make',\n",
              "  'too',\n",
              "  'putt',\n",
              "  'era',\n",
              "  'should',\n",
              "  'turn',\n",
              "  'cryptic',\n",
              "  'dialogue',\n",
              "  'shakespeare',\n",
              "  'easy',\n",
              "  'grader',\n",
              "  'technical',\n",
              "  'level',\n",
              "  'well',\n",
              "  'think',\n",
              "  'good',\n",
              "  'cinematography',\n",
              "  'future',\n",
              "  'great',\n",
              "  'vilmos',\n",
              "  'zsigmond',\n",
              "  'future',\n",
              "  'star',\n",
              "  'sally',\n",
              "  'kirkland',\n",
              "  'frederic',\n",
              "  'forrest',\n",
              "  'see',\n",
              "  'briefly']]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmQTBBGbEDQq"
      },
      "source": [
        "# Word Embeddings with Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKmj_slrEGnt",
        "outputId": "d461537c-b97e-4a6b-93c8-d95c11a2857a"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "embedding_dimension = 100\n",
        "\n",
        "model = Word2Vec(reviews,size=embedding_dimension, window=3, min_count=3, workers=4)\n",
        "\n",
        "print(model.sg)\n",
        "\n",
        "word_vectors = model.wv\n",
        "\n",
        "del model\n",
        "\n",
        "len(word_vectors.vocab)\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28163"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRkV_CYBIEdF",
        "outputId": "65c9c5fd-a17c-49b2-9886-5957d797567f"
      },
      "source": [
        "word_vectors.similar_by_word(word=\"good\", topn=5)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('decent', 0.7125661373138428),\n",
              " ('alright', 0.6783406734466553),\n",
              " ('darn', 0.6632038354873657),\n",
              " ('okay', 0.6426970958709717),\n",
              " ('great', 0.6315128803253174)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXjLAbRXIKGJ",
        "outputId": "ec8861bb-c849-4698-8d66-bf6eeafaa820"
      },
      "source": [
        "word_vectors.similar_by_word(word=\"bad\", topn=5)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('horrible', 0.695655345916748),\n",
              " ('terrible', 0.6865178346633911),\n",
              " ('suck', 0.6831722259521484),\n",
              " ('lousy', 0.671484112739563),\n",
              " ('lame', 0.6639829874038696)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdTlemQhIKUm",
        "outputId": "292969f9-ee6f-483c-df7b-87190c4dc6e1"
      },
      "source": [
        "word_vectors.most_similar(positive=\"bad\",topn=4)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('horrible', 0.695655345916748),\n",
              " ('terrible', 0.6865178346633911),\n",
              " ('suck', 0.6831722259521484),\n",
              " ('lousy', 0.671484112739563)]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bby6oE3IKXw",
        "outputId": "dc3689bb-c8b0-4edc-dbed-480df2fa8c3a"
      },
      "source": [
        "word_vectors.similarity(\"good\",\"bad\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5675432"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxL9DsC-IKaV",
        "outputId": "bbf99dba-f07a-4a82-f427-4d10e9b0dee8"
      },
      "source": [
        "word_vectors.similarity(\"good\",\"be\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.27097836"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLHdFdXZIKcW",
        "outputId": "a8521e4f-1a19-4753-c38a-999d5f306966"
      },
      "source": [
        "word_vectors.similar_by_word(word=\"school\", topn=5)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('college', 0.759829580783844),\n",
              " ('class', 0.752888560295105),\n",
              " ('schooler', 0.7477148771286011),\n",
              " ('bromwell', 0.7114781141281128),\n",
              " ('student', 0.7080619931221008)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCA-GRFIIKfK",
        "outputId": "cfd0f78b-d398-46db-a6e5-432a75ebeec9"
      },
      "source": [
        "word_vectors.similar_by_word(word=\"comedy\", topn=5)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('slapstick', 0.6919984817504883),\n",
              " ('satire', 0.6785503625869751),\n",
              " ('farce', 0.6700247526168823),\n",
              " ('humor', 0.6629171371459961),\n",
              " ('parody', 0.6436172127723694)]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihMUSAkPIVr2",
        "outputId": "d2a75b21-3bf2-4db9-ceee-f61ca8452a94"
      },
      "source": [
        "word_vectors.similar_by_word(word=\"action\", topn=5)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('suspense', 0.635307788848877),\n",
              " ('thrill', 0.5930182933807373),\n",
              " ('blacksploitation', 0.5928084850311279),\n",
              " ('excitement', 0.5843026041984558),\n",
              " ('unexpecting', 0.5628532767295837)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPZnrINPIV3q"
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTp8KXpIPD-5"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC0rRQBRIWBV"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "SEED = 2222\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlKEBPP-SuU7"
      },
      "source": [
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvnfQO9SSug_"
      },
      "source": [
        "def word2idx(embedding_model,review):\n",
        "    index_review = []\n",
        "    for word in review:\n",
        "        try:\n",
        "            index_review.append(embedding_model.vocab[word].index)\n",
        "        except: \n",
        "             pass\n",
        "    return torch.tensor(index_review)\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBy59WdQSuuU",
        "outputId": "d12da45e-3fa9-4081-e9f4-3ef59bfe422b"
      },
      "source": [
        "\n",
        "padding_value = len(word_vectors.index2word)\n",
        "padding_value"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28163"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRkh1EqPSu6T"
      },
      "source": [
        "index_review = list(map(lambda review: word2idx(word_vectors,review),reviews))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTBjyFAgSvE5"
      },
      "source": [
        "embedding_weights = torch.Tensor(word_vectors.vectors)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6xAuSj7SvQ1"
      },
      "source": [
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout,embedding_weights):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_weights)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, text_lengths):\n",
        "        #x [sent length , batch size]\n",
        "        embedded = self.embedding(x) #[sentect len,batch size,embedding dim]\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)#output[sent length,batch size,hiddendin*num of directions],[numberlayers*num of dir,batch size,hid dim]\n",
        "        #[f0,b0,f1,b1,.......fn,bn]\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden.squeeze(0))\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNL3k3yPSvZi"
      },
      "source": [
        "INPUT_DIM = padding_value\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqDAXDLCTHgI",
        "outputId": "cadeeaba-8111-44e9-bb6e-6b3935279bff"
      },
      "source": [
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, embedding_weights)\n",
        "model"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(28163, 100)\n",
              "  (rnn): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmSbOOr4TQTJ"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SffllJNLTQWJ",
        "outputId": "7c67794a-1810-4c3e-d879-1b8510c364e4"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "device.type"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haNgaZ_FTQY8",
        "outputId": "0e12afa0-1249-4dcc-bc4d-04406b3ef7fe"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "labels[:5]\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['positive', 'negative', 'positive', 'negative', 'positive']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-Pd2tQ0TQbI",
        "outputId": "471c5170-589a-408b-e715-52f747e5157b"
      },
      "source": [
        "\n",
        "labels_num = [0 if label == 'negative' else 1 for label in labels]\n",
        "labels_num[:5]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 1, 0, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlWnQB0oTQft",
        "outputId": "6713960e-c5ec-421a-d247-09b540726de4"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(index_review, labels_num, test_size=0.2)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "print(len(X_train),len(X_test),len(X_val))\n",
        "\n",
        "print(len(y_train),len(y_test),len(y_val))\n",
        "\n",
        "batch_size = 128 \n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16000 5000 4000\n",
            "16000 5000 4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv5aM6dQUL2g"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TT4S-mHUToX"
      },
      "source": [
        "def iterator_func(X,y):\n",
        "    size = len(X)\n",
        "    permutation = np.random.permutation(size)\n",
        "    iterator = []\n",
        "    for i in range(0,size, batch_size):\n",
        "        indices = permutation[i:i+batch_size]\n",
        "        batch = {}\n",
        "        batch[\"text\"] = [X[i] for i in indices]\n",
        "        batch[\"label\"] = [y[i] for i in indices]\n",
        "        \n",
        "        batch[\"text\"],batch[\"label\"] = zip(*sorted(zip(batch[\"text\"],batch[\"label\"]),key=lambda x: len(x[0]),reverse=True))\n",
        "        batch[\"length\"] = [len(review) for review in batch[\"text\"]]\n",
        "        batch[\"length\"] = torch.IntTensor(batch[\"length\"])\n",
        "        batch[\"text\"] = torch.nn.utils.rnn.pad_sequence(batch[\"text\"],batch_first=True).t()\n",
        "        batch[\"label\"] = torch.Tensor(batch[\"label\"])\n",
        "        \n",
        "        batch[\"label\"]  = batch[\"label\"].to(device)\n",
        "        batch[\"length\"] = batch[\"length\"].to(device) \n",
        "        batch[\"text\"]   = batch[\"text\"].to(device) \n",
        "        \n",
        "        iterator.append(batch)\n",
        "        \n",
        "    return iterator\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7AF7p9DUT28",
        "outputId": "898430b8-b0da-44e1-90f6-6f4059db7102"
      },
      "source": [
        "train_iterator = iterator_func(X_train,y_train)\n",
        "valid_iterator = iterator_func(X_val,y_val)\n",
        "test_iterator = iterator_func(X_test,y_test)\n",
        "\n",
        "print(len(train_iterator),len(test_iterator),len(valid_iterator))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125 40 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyVR_ndGUUC_"
      },
      "source": [
        "\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbt38KumUUMA"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum()/len(correct)\n",
        "    return acc\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch[\"text\"],batch[\"length\"]).squeeze(1)\n",
        "        loss = criterion(predictions, batch[\"label\"])\n",
        "        acc = binary_accuracy(predictions, batch[\"label\"])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            predictions = model(batch[\"text\"],batch[\"length\"]).squeeze(1)\n",
        "            loss = criterion(predictions, batch[\"label\"])\n",
        "            acc = binary_accuracy(predictions, batch[\"label\"])\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "xVbb9KHmZ7oV",
        "outputId": "f54e44da-b161-4102-95b5-e6722343e115"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-163fffd27d2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-fc4740cc02b5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mepoch_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdeXSXZiZ7to",
        "outputId": "f981d85e-408d-4eeb-9056-70196cd2962b"
      },
      "source": [
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 0.372 | Test Acc: 83.44% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKicUGdJUoet",
        "outputId": "1346ce77-a859-4ed4-ec25-c7abb6b4992f"
      },
      "source": [
        "def predict_sentiment(sentence):\n",
        "    tokenized = pipeline(sentence)\n",
        "    indexed = word2idx(word_vectors,tokenized)\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    prediction = torch.sigmoid(model(tensor,torch.LongTensor([len(indexed)]).to(device)))\n",
        "    return prediction.item()\n",
        "\n",
        "print(predict_sentiment(\"this is a awesome movie\"))\n",
        "\n",
        "print(predict_sentiment(\"this is not an action movie, is a  very good movie\"))\n",
        "\n",
        "print(predict_sentiment(\"this is comedy movie\"))\n",
        "\n",
        "print(predict_sentiment(\"this is an awful movie\"))\n",
        "\n",
        "print(predict_sentiment(\"this is a bad movie\"))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7221322059631348\n",
            "0.5807857513427734\n",
            "0.7732258439064026\n",
            "0.03303910419344902\n",
            "0.05560372397303581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tos1qqd2UopZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_sKa29sdsFh"
      },
      "source": [
        "# Now With LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOccdmaGdctz"
      },
      "source": [
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout,embedding_weights):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_weights)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, text_lengths):\n",
        "        #x [sent length , batch size]\n",
        "        embedded = self.embedding(x) #[sentect len,batch size,embedding dim]\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)#output[sent length,batch size,hiddendin*num of directions],[numberlayers*num of dir,batch size,hid dim]\n",
        "        #[f0,b0,f1,b1,.......fn,bn]\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EepiQUudczj"
      },
      "source": [
        "NPUT_DIM = padding_value\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vL4MseoerzN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ5jF6X7dc2o",
        "outputId": "c54a81f9-9034-4802-a7e3-0d30740239fe"
      },
      "source": [
        "model_with_lstm = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, embedding_weights)\n",
        "\n",
        "model_with_lstm"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(28163, 100)\n",
              "  (rnn): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH7eqDaZesl5"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "device.type\n",
        "\n",
        "model_with_lstm = model_with_lstm.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2q23rwjdc5Y"
      },
      "source": [
        "optimizer = optim.Adam(model_with_lstm.parameters())\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXsR5vI6dc7x",
        "outputId": "7e7495c0-0ba5-4983-ba6d-6beaf0d09c51"
      },
      "source": [
        "\n",
        "N_EPOCHS = 5\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train(model_with_lstm, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model_with_lstm, valid_iterator, criterion)\n",
        "    \n",
        "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
        "\n",
        "\n",
        "test_loss, test_acc = evaluate(model_with_lstm, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch: 01 | Train Loss: 0.401 | Train Acc: 82.20% | Val. Loss: 0.400 | Val. Acc: 82.74% |\n",
            "| Epoch: 02 | Train Loss: 0.381 | Train Acc: 83.52% | Val. Loss: 0.363 | Val. Acc: 83.54% |\n",
            "| Epoch: 03 | Train Loss: 0.345 | Train Acc: 84.97% | Val. Loss: 0.319 | Val. Acc: 86.84% |\n",
            "| Epoch: 04 | Train Loss: 0.321 | Train Acc: 86.28% | Val. Loss: 0.297 | Val. Acc: 87.21% |\n",
            "| Epoch: 05 | Train Loss: 0.312 | Train Acc: 87.00% | Val. Loss: 0.301 | Val. Acc: 87.43% |\n",
            "| Test Loss: 0.316 | Test Acc: 86.89% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss9QI7wMdc_Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APjy2Z-sUor3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}